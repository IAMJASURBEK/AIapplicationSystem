{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfocLon2NSwhcybVvr1nXd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAMJASURBEK/AIapplicationSystem/blob/main/week3_jasurbek.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9oYbbnWJ904O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(3) # To make repeatable LEARNING_RATE = 0.1\n",
        "index_list = [0, 1, 2, 3] # Used to randomize order\n",
        "# Define training examples.\n",
        "x_train = [np.array([1.0, -1.0, -1.0]),\n",
        "np.array([1.0, 1.0, -1.0]),\n",
        "np.array([1.0, 1.0, 1.0])]\n",
        "y_train = [0.0, 1.0, 1.0, 0.0] # Output (ground truth)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def neuron_w(input_count):\n",
        "  weights = np.zeros(input_count + 1)\n",
        "  for i in range(1,(input_count+1)):\n",
        "    weights[i] = np.random.uniform(-1.0, 1.0)\n",
        "    return weights\n",
        "n_w = [neuron_w(2), neuron_w(2), neuron_w(2)] \n",
        "n_y = [0, 0, 0]\n",
        "n_error = [0, 0, 0]\n"
      ],
      "metadata": {
        "id": "99kGqz2-9_UW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_learning():\n",
        "  print(\"Current weights:\")\n",
        "  for i, w in enumerate(n_w):\n",
        "    print('neuron ', i,\n",
        "     \": w0 =\", \"%5.2f\" % w[0],\n",
        "     \", w1 =\", \"%5.2f\" % w[1], \", w2 =\",\n",
        "     \"%5.2f\" % w[2])\n",
        "  print('----------------')"
      ],
      "metadata": {
        "id": "kT88XEpd-A5r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_pass(x):\n",
        "  global n_y\n",
        "  n_y[0] = np.tanh(np.dot(n_w[0], x)) # Neuron 0\n",
        "  n_y[1] = np.tanh(np.dot(n_w[1], x)) # Neuron 1\n",
        "  n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # 1.0 is bias\n",
        "  z2 = np.dot(n_w[2], n2_inputs)\n",
        "  n_y[2] = 1.0 / (1.0 + np.exp(-z2))"
      ],
      "metadata": {
        "id": "eGgOh9dv-Dvn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(y_truth): \n",
        "  global n_error\n",
        "  error_prime = -(y_truth - n_y[2]) # Derivative of loss-func\n",
        "  derivative = n_y[2] * (1.0 - n_y[2]) # Logistic derivative\n",
        "  n_error[2] = error_prime * derivative\n",
        "  derivative = 1.0 - n_y[0]**2 # tanh derivative\n",
        "  n_error[0] = n_w[2][1] * n_error[2] * derivative\n",
        "  derivative = 1.0 - n_y[1]**2 # tanh derivative\n",
        "  n_error[1] = n_w[2][2] * n_error[2] * derivative"
      ],
      "metadata": {
        "id": "t7HtkovZ-GUb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adjust_weights(x):\n",
        "  global n_w\n",
        "  n_w[0] -= (x * LEARNING_RATE * n_error[0])\n",
        "  n_w[1] -= (x * LEARNING_RATE * n_error[1])\n",
        "  n2_inputs = np.array([1.0, n_y[0], n_y[1]]) # 1.0 is bias\n",
        "  n_w[2] -= (n2_inputs * LEARNING_RATE * n_error[2])"
      ],
      "metadata": {
        "id": "8tDFFZhI-I6j"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}