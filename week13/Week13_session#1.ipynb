{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IAMJASURBEK/AIapplicationSystem/blob/main/week13/Week13_session%231.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the TextVectorization layer"
      ],
      "metadata": {
        "id": "3-BDIIgCBgKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CfmJbcXCBVK_"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    \n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLco_KKbCvil",
        "outputId": "472b35f0-fb70-4b81-8dfd-918f25a316dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meXYLsIrED9y",
        "outputId": "a257d248-04b9-4193-e7f5-af338066a2bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = \"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "AA9aOX_VEU2s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = \"int\",\n",
        "    standardize = custom_standardization_fn,\n",
        "    split = custom_split_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "x4EP6SMEEfER"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "sFOAe2I1FWC6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Displaying the vocabulary**"
      ],
      "metadata": {
        "id": "ac_9youZH7wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm69spFOHuOo",
        "outputId": "167db79a-757d-43df-b386-aa3f0b0ab6b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hra_QdOTIHHn",
        "outputId": "b81a07d8-812a-420a-e7d8-243450d7f551"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ],
      "metadata": {
        "id": "Kl_gIQjGKMMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing the IMDB movie reviews data"
      ],
      "metadata": {
        "id": "GKHIVQb3KRme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQbJCuKeKXCN",
        "outputId": "a116b968-ea05-4cd6-b0e3-fca94c6bd1dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  10.3M      0  0:00:07  0:00:07 --:--:-- 16.1M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "cXdoOOQNM-CZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L842nUo7ND8-",
        "outputId": "3ef8ca3b-0eb4-4c26-a0ae-b70155c64217"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname, val_dir / category / fname)"
      ],
      "metadata": {
        "id": "PjoyQpwfPepS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size = batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size = batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size = batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0pha26sgG9T",
        "outputId": "0b1d5f0c-1090-4387-8b4b-a2f48eecbae7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying the shapes and dtypes of the first batch"
      ],
      "metadata": {
        "id": "kBB2UPDfgmJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30BQPke6glpe",
        "outputId": "ee9ec49c-ac13-43e6-9609-64001fea4459"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor(b\"This film was choppy, incoherent and contrived. It was also an extremely mean-spirited portrayal of women. I rented it because it was listed as a comedy (that's a stretch), and because the cover said Andie McDowell was acting up a storm in it. She wasn't. I'm a gal, I watched this film with two guys, and we spent an hour afterwards exclaiming over how bad it was.<br /><br />WARNING: PLOT SUMMARY BELOW! RAMPANT SPOILERS!<br /><br />The movie starts out with a fairly hackneyed plot about an older woman who takes up with a younger man, to the severe disapproval of her two jealous single girlfriends. They want her to marry a boring guy their own age who is kind of in love with her. But she's so happy with her oversexed puppy that you're rooting for them to stick it out, and sure enough, she decides to marry the guy. But her harpy girlfriend, aided by the wishy-washy one, sets up a plot to trick our heroine into thinking the guy is cheating on her. It works. She has a fight with him, he runs out of the house and is crushed by a truck (Remember the movie's title?) So now he's dead, two-thirds of the way through the film. And although our heroine is a school headmistress who spends her time watching over girls, she apparently forgot to use birth control and is pregnant.<br /><br />She's already broken off relations with her girlfriends, because they were so unsupportive. Alone and pitiful, she decides to marry the boring guy. Did I mention that the boring guy who kind of loves her is a minister? She had asked him to marry her to the young guy (nice, huh?), but now she tells him she'll marry him, and apparently he has no objections to being dicked around in this fashion. But her girlfriends rescue her at the altar and take her home, where they not-quite-confess that they were mostly responsible for the love of her life getting smushed. She has the kid. In the final scene, they leave it in a crib inside her house while they go out on the porch to drink, smoke and be smug. I kid you not, it's that bad. I left out the part about the cancer red-herring and the harpy's ridiculous lesbian moment.\", shape=(), dtype=string)\n",
            "targets[0] tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Processing words as a set: The bag-of-words approach"
      ],
      "metadata": {
        "id": "Ntyx_LO4jK2t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single words (unigrams) with binary encoding"
      ],
      "metadata": {
        "id": "8SeQ51TAjQOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing our datasets with a TextVectorization layer\n",
        "\n"
      ],
      "metadata": {
        "id": "XmY4fYwQjsU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\",\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")"
      ],
      "metadata": {
        "id": "1Kd0i-D0jP5w"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the output of our binary unigram dataset"
      ],
      "metadata": {
        "id": "vBGHltFBmYgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSej8LkQmdoG",
        "outputId": "b31ddd4a-7f57-4f27-e126-fb17e5e60c41"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0] tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our model-building utility"
      ],
      "metadata": {
        "id": "THDvTVMunbZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
        "  inputs = keras.Input(shape = (max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "befBpN8rng_a"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing the binary unigram model"
      ],
      "metadata": {
        "id": "MyVVKVp2pT3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only = True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data = binary_1gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opszT4zupYCK",
        "outputId": "5c484a1e-f833-4941-caca-c55bb0ffabf4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 11s 13ms/step - loss: 0.4181 - accuracy: 0.8238 - val_loss: 0.3057 - val_accuracy: 0.8796\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2837 - accuracy: 0.8932 - val_loss: 0.2958 - val_accuracy: 0.8860\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2509 - accuracy: 0.9125 - val_loss: 0.3035 - val_accuracy: 0.8874\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2428 - accuracy: 0.9176 - val_loss: 0.3178 - val_accuracy: 0.8898\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2244 - accuracy: 0.9234 - val_loss: 0.3391 - val_accuracy: 0.8870\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2202 - accuracy: 0.9280 - val_loss: 0.3392 - val_accuracy: 0.8890\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2232 - accuracy: 0.9311 - val_loss: 0.3580 - val_accuracy: 0.8922\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.2114 - accuracy: 0.9363 - val_loss: 0.3689 - val_accuracy: 0.8912\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2168 - accuracy: 0.9368 - val_loss: 0.3699 - val_accuracy: 0.8890\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2106 - accuracy: 0.9388 - val_loss: 0.3774 - val_accuracy: 0.8854\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.2901 - accuracy: 0.8905\n",
            "Test acc: 0.891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigrams with binary encoding"
      ],
      "metadata": {
        "id": "OMw8yhAGqD05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the TextVectorization layer to return bigrams"
      ],
      "metadata": {
        "id": "JpX3uv9RqI09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "IPJ21phmqII0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing the binary bigram model"
      ],
      "metadata": {
        "id": "AQl67EBVrGL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data = binary_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kDg4J4orKnp",
        "outputId": "e4daf729-1411-44ab-ee7b-3bc96e6e4ecd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 15ms/step - loss: 0.3729 - accuracy: 0.8474 - val_loss: 0.2798 - val_accuracy: 0.8902\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2361 - accuracy: 0.9194 - val_loss: 0.2790 - val_accuracy: 0.8970\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2070 - accuracy: 0.9349 - val_loss: 0.3009 - val_accuracy: 0.8922\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1983 - accuracy: 0.9420 - val_loss: 0.3163 - val_accuracy: 0.8928\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1816 - accuracy: 0.9468 - val_loss: 0.3366 - val_accuracy: 0.8940\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.1832 - accuracy: 0.9512 - val_loss: 0.3464 - val_accuracy: 0.8966\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 6ms/step - loss: 0.1780 - accuracy: 0.9516 - val_loss: 0.3625 - val_accuracy: 0.8928\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1830 - accuracy: 0.9541 - val_loss: 0.3692 - val_accuracy: 0.8912\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 6ms/step - loss: 0.1758 - accuracy: 0.9535 - val_loss: 0.3693 - val_accuracy: 0.8906\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1713 - accuracy: 0.9557 - val_loss: 0.3750 - val_accuracy: 0.8914\n",
            "782/782 [==============================] - 10s 12ms/step - loss: 0.2690 - accuracy: 0.9024\n",
            "Test acc: 0.902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bigrams with TF-IDF encoding"
      ],
      "metadata": {
        "id": "AVEo4PGpsftX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring the TextVectorization layer to return token counts"
      ],
      "metadata": {
        "id": "Efh5DpQxsjB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"count\",\n",
        ")"
      ],
      "metadata": {
        "id": "4Y9VMb75s02_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuring TextVectorization to return TF-IDF-weighted outputs"
      ],
      "metadata": {
        "id": "G-eQijzos0nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "mGOTH70dtCZ6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ],
      "metadata": {
        "id": "OVqZcpSqthTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data = tfidf_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61YeC1AVtlsS",
        "outputId": "0567346f-8ad3-4135-a9f9-bece6690c913"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 15ms/step - loss: 0.5047 - accuracy: 0.7513 - val_loss: 0.3101 - val_accuracy: 0.8844\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3705 - accuracy: 0.8283 - val_loss: 0.3074 - val_accuracy: 0.8740\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3407 - accuracy: 0.8450 - val_loss: 0.3187 - val_accuracy: 0.8666\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3324 - accuracy: 0.8500 - val_loss: 0.3556 - val_accuracy: 0.8640\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.3270 - accuracy: 0.8510 - val_loss: 0.3313 - val_accuracy: 0.8610\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3116 - accuracy: 0.8586 - val_loss: 0.3493 - val_accuracy: 0.8516\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2819 - accuracy: 0.8741 - val_loss: 0.3474 - val_accuracy: 0.8688\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2611 - accuracy: 0.8830 - val_loss: 0.3645 - val_accuracy: 0.8566\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2508 - accuracy: 0.8928 - val_loss: 0.3608 - val_accuracy: 0.8676\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2524 - accuracy: 0.8967 - val_loss: 0.3646 - val_accuracy: 0.8588\n",
            "782/782 [==============================] - 8s 10ms/step - loss: 0.3050 - accuracy: 0.8726\n",
            "Test acc: 0.873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape = (1,), dtype = \"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "2-TjD7YKuXd8"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkbbcLpHuwnr",
        "outputId": "ed769063-d427-43be-f123-a660b5a63b89"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94.25 percent positive\n"
          ]
        }
      ]
    }
  ]
}